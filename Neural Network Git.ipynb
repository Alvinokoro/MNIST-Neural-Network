{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b091b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "#This function is actually part of the TensorFlow/Keras library, not scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8700c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#The MNIST dataset is a widely used collection of handwritten digits, commonly used for training and testing machine learning models, especially in the field of computer vision and deep learning. \n",
    "#Here's a more detailed explanation:\n",
    "#Dataset Content:\n",
    "#It contains 70,000 images of handwritten digits (0-9). Each image is a 28x28 pixel grayscale image. The dataset is split into 60,000 training images and 10,000 test images.\n",
    "#x_train: 60,000 training images (shape: (60000, 28, 28)) y_train: 60,000 labels for the training images (shape: (60000,)) x_test: 10,000 test images (shape: (10000, 28, 28)) y_test: 10,000 labels for the test images (shape: (10000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313bf72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (60000, 28, 28)\n",
      "Original value range: 0 - 255\n",
      "Normalized value range: 0.0 - 1.0\n",
      "Reshaped for dense layer: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Original shape and value range\n",
    "print(\"Original shape:\", x_train.shape)\n",
    "print(\"Original value range:\", np.min(x_train), \"-\", np.max(x_train))\n",
    "\n",
    "# Normalize\n",
    "x_train_normalized = x_train.astype('float32') / 255\n",
    "x_test_normalized = x_test.astype('float32') / 255 \n",
    "print(\"Normalized value range:\", np.min(x_train_normalized), \"-\", np.max(x_train_normalized))\n",
    "\n",
    "# Reshape for a dense neural network\n",
    "x_train_reshaped = x_train_normalized.reshape(-1, 784)\n",
    "x_test_reshaped = x_test_normalized.reshape(-1, 784)\n",
    "print(\"Reshaped for dense layer:\", x_train_reshaped.shape)\n",
    "#Your input data (likely MNIST images) is probably in a format like (60000, 28, 28) - meaning 60000 images that are 28x28 pixels\n",
    "#A dense/fully connected layer expects a single flat vector as input, not a 2D image\n",
    "#The reshape operation flattens each 28x28 image into a single vector of 784 pixels (28 * 28 = 784)\n",
    "\n",
    "\n",
    "#The -1 in detail:  If we have 60,000 training images, the -1 will automatically become 60,000.\n",
    "#The -1 is a special placeholder that tells numpy \"figure out what this dimension should be to make everything fit.\"\n",
    "\n",
    "#So you're essentially converting each 2D image matrix into a 1D array that your neural network can process. Think of it like taking a grid of pixels and laying them out in one long line.\n",
    "\n",
    "#You need to normalize and reshape x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43410c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 36000 samples (60.0%)\n",
      "Validation set: 12000 samples (20.0%)\n",
      "Test set: 12000 samples (20.0%)\n"
     ]
    }
   ],
   "source": [
    "#If you want both validation and test sets to be 20% each (leaving 60% for training), you'll need to do two splits. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# First split: Separate out the test set (20%)\n",
    "x_temp, x_test, y_temp, y_test = train_test_split(x_train_reshaped , y_train, \n",
    "                                                 test_size=0.2, \n",
    "                                                 random_state=42)\n",
    "\n",
    "# Second split: Split remaining data into training (75%) and validation (25%)\n",
    "# 75% of 80% ≈ 60% of original data\n",
    "# 25% of 80% ≈ 20% of original data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, \n",
    "                                                 test_size=0.25, \n",
    "                                                 random_state=42)\n",
    "\n",
    "# Let's verify the sizes\n",
    "print(f\"Training set: {len(x_train)} samples ({len(x_train)/len(x_train_normalized)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(x_val)} samples ({len(x_val)/len(x_train_normalized)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(x_test)} samples ({len(x_test)/len(x_train_normalized)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd8774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 1 with architecture: [32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALVIN OKORO-IJAGHA\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.7043 - val_loss: 0.2599\n",
      "Epoch 2/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.2489 - val_loss: 0.2168\n",
      "Epoch 3/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.1870 - val_loss: 0.1907\n",
      "Epoch 4/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.1654 - val_loss: 0.1753\n",
      "Epoch 5/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.1428 - val_loss: 0.1716\n",
      "\n",
      "Training Model 2 with architecture: [64, 32]\n",
      "Epoch 1/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.6321 - val_loss: 0.1987\n",
      "Epoch 2/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1765 - val_loss: 0.1630\n",
      "Epoch 3/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.1250 - val_loss: 0.1407\n",
      "Epoch 4/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0961 - val_loss: 0.1205\n",
      "Epoch 5/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0732 - val_loss: 0.1123\n",
      "\n",
      "Training Model 3 with architecture: [128, 64, 32]\n",
      "Epoch 1/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.5717 - val_loss: 0.1637\n",
      "Epoch 2/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.1370 - val_loss: 0.1255\n",
      "Epoch 3/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0883 - val_loss: 0.1173\n",
      "Epoch 4/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0706 - val_loss: 0.1056\n",
      "Epoch 5/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0556 - val_loss: 0.0985\n",
      "\n",
      "Training Model 4 with architecture: [256, 128]\n",
      "Epoch 1/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.4459 - val_loss: 0.1361\n",
      "Epoch 2/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.1072 - val_loss: 0.1093\n",
      "Epoch 3/5\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - loss: 0.0691 - val_loss: 0.0964\n",
      "Epoch 4/5\n",
      "\u001b[1m 609/1125\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0437"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "models = []\n",
    "cv_errors = []\n",
    "\n",
    "# Define different architectures to try\n",
    "# Each list represents the number of units in each hidden layer\n",
    "architectures = [\n",
    "  [32],  # Single hidden layer with 32 units\n",
    "[64, 32],  # Two hidden layers: 64 units, then 32 units\n",
    "[128, 64, 32],  # Three hidden layers: 128, 64, then 32 units\n",
    "[256, 128],  # Two hidden layers: 256 units, then 128 units\n",
    "[512, 256, 128],  # Three hidden layers: 512, 256, then 128 units\n",
    "[64, 64, 64],  # Three hidden layers with 64 units each\n",
    "[256],  # Single hidden layer with 256 units\n",
    "[128, 64],  # Two hidden layers: 128 units, then 64 units\n",
    "[512, 256, 128, 64],  # Four hidden layers: 512, 256, 128, then 64 units\n",
    "[1024, 512]  # Two hidden layers: 1024 units, then 512 units\n",
    "]\n",
    "\n",
    "for i, hidden_units in enumerate(architectures):\n",
    "    print(f\"\\nTraining Model {i+1} with architecture: {hidden_units}\")\n",
    "    \n",
    "    # Build model with specified architecture\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    #model.add(tf.keras.Input(shape=(784,)))\n",
    "    \n",
    "    #or\n",
    "    model.add(Dense(hidden_units[0], activation='relu', input_shape=(784,)))\n",
    "#so in our model selection , this line of code does two important things, it would add the first dense layer as well as the input to the model \n",
    "#creates the input layer: input_shape=(784,) tells the model to expect inputs of 784 features (our flattened 28×28 images)\n",
    "#This automatically sets up the input layer though we don't explicitly write it\n",
    "#The hidden_units[0] with relu; this layer will be fully connected to all 784 input features\n",
    "\n",
    "\n",
    "#The model always has an input layer (784 units for MNIST) and an output layer (10 units for MNIST).\n",
    "#So a model with architecture [32] actually has three layers total: input (784) -> hidden (32) -> output (10).  \n",
    "#The input layer in this code isn't explicitly defined with Dense(). Instead, it's implicitly defined by the input_shape parameter in the first Dense layer.\n",
    "\n",
    "# it's because the neurons are interconnected with the input shape\n",
    "#So, 784 is not the number of units in a layer, but the number of input features. The actual number of units in the first hidden layer is hidden_units[0].\n",
    "#so it shows that the first hidden layer is directly connect to the input x(vector)\n",
    "#This direct connection allows the first hidden layer to learn features directly from the raw input data. Subsequent layers then learn higher-level features based on the outputs of the previous layer.\n",
    " # Hidden layers\n",
    "    for units in hidden_units[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "    \n",
    "    # Output layer (always 10 units for MNIST)\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(x_train, y_train,\n",
    "                       validation_data=(x_val, y_val),\n",
    "                       epochs=5,\n",
    "                       verbose=1)\n",
    "#Validation data (x_val/y_val) is like a practice test,the model doesn't learn from validation data, but uses it to check its performance\n",
    "#This helps detect overfitting - if the model does great on training data but poorly on validation data, it's memorizing instead of learning\n",
    "#With verbose=1, you get a progress bar and regular updates on loss and metrics for each epoch.\n",
    " \n",
    "    # Store validation error\n",
    "    cv_error = history.history['val_loss'][-1] # from the history created under fit, we want to get the last val loss when we run the code\n",
    "    cv_errors.append(cv_error)\n",
    "    models.append(model)\n",
    "    \n",
    "#history.history is a dictionary containing lists of metrics (like loss, accuracy, val_loss, etc.)\n",
    "#history.history['val_loss'] gives us the list of validation losses for each epoch.\n",
    "#The -1 index grabs the last value from that list\n",
    "#Later, you use these collected errors to determine which model performed best on the validation data.\n",
    "\n",
    "# Find best model\n",
    "best_model_index = np.argmin(cv_errors)\n",
    "best_model = models[best_model_index]\n",
    "\n",
    "#np.argmin() returns the index of the minimum value in an array. So this is identifying which model performed best on the validation data.\n",
    "\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Best model was architecture: {architectures[best_model_index]}\")\n",
    "print(f\"Best validation error: {cv_errors[best_model_index]:.4f}\")\n",
    "\n",
    "# Make sure cv_errors is not empty\n",
    "if len(cv_errors) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(cv_errors)), cv_errors)\n",
    "    plt.xlabel('Model Architecture')\n",
    "    plt.ylabel('Validation Error')\n",
    "    plt.title('Model Selection: Validation Error vs Architecture')\n",
    "    plt.xticks(range(len(cv_errors)), [f\"Model {i+1}\" for i in range(len(cv_errors))])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"cv_errors is empty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3962eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Simpler model with only L1 regularization\n",
    "model = Sequential([\n",
    "   tf.keras.Input(shape=(784,)),\n",
    "   # First hidden layer with L1 only\n",
    "   Dense(128, activation='relu', \n",
    "         kernel_regularizer=l1(0.001)),\n",
    "   # Second hidden layer\n",
    "   Dense(64, activation='relu',\n",
    "         kernel_regularizer=l1(0.001)),\n",
    "   # Output layer\n",
    "   Dense(10, activation='linear',\n",
    "         kernel_regularizer=l1(0.001))\n",
    "])\n",
    "\n",
    "# Compile with lower learning rate\n",
    "model.compile(\n",
    "   optimizer=Adam(learning_rate=1e-4),\n",
    "   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "# Train without early stopping\n",
    "history = model.fit(\n",
    "   x_train, \n",
    "   y_train,\n",
    "   validation_data=(x_val, y_val),\n",
    "   epochs=35,\n",
    "   batch_size=32,\n",
    "   verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions\n",
    "y_pred_prob = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Create and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Example of making predictions on a few test images\n",
    "n_samples = 5\n",
    "x_samples = x_test[:n_samples]\n",
    "predictions = model.predict(x_samples)\n",
    "predicted_digits = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Visualize the predictions\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(n_samples):\n",
    "    plt.subplot(1, n_samples, i+1)\n",
    "    plt.imshow(x_samples[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Predicted: {predicted_digits[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9a662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
